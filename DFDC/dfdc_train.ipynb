{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"{}\".format(2) # gpu idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pylab import *\n",
    "from PIL import Image, ImageChops, ImageEnhance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiho/.venv/py369tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jiho/.venv/py369tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jiho/.venv/py369tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jiho/.venv/py369tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jiho/.venv/py369tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jiho/.venv/py369tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_frame_folder = 'train_sample_videos'\\nwith open(os.path.join(train_frame_folder, 'metadata.json'), 'r') as file:\\n    data = json.load(file)\\nlist_of_train_data = [f for f in os.listdir(train_frame_folder) if f.endswith('.mp4')]\\ndetector = dlib.get_frontal_face_detector()\\nfor vid in list_of_train_data:\\n    count = 0\\n    cap = cv2.VideoCapture(os.path.join(train_frame_folder, vid))\\n    frameRate = cap.get(5)\\n    while cap.isOpened():\\n        frameId = cap.get(1)\\n        ret, frame = cap.read()\\n        if ret != True:\\n            break\\n        if frameId % ((int(frameRate)+1)*1) == 0:\\n            face_rects, scores, idx = detector.run(frame, 0)\\n            for i, d in enumerate(face_rects):\\n                x1 = d.left()\\n                y1 = d.top()\\n                x2 = d.right()\\n                y2 = d.bottom()\\n                crop_img = frame[y1:y2, x1:x2]\\n                if data[vid]['label'] == 'REAL':\\n                    cv2.imwrite('dataset/real/'+vid.split('.')[0]+'_'+str(count)+'.png', cv2.resize(crop_img, (128, 128)))\\n                elif data[vid]['label'] == 'FAKE':\\n                    cv2.imwrite('dataset/fake/'+vid.split('.')[0]+'_'+str(count)+'.png', cv2.resize(crop_img, (128, 128)))\\n                count+=1\\n                \""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#frame추출->png형식으로 저장(real, fake폴더에 각각 저장)\n",
    "train_frame_folder = 'train_sample_videos'\n",
    "with open(os.path.join(train_frame_folder, 'metadata.json'), 'r') as file:\n",
    "    data = json.load(file)\n",
    "list_of_train_data = [f for f in os.listdir(train_frame_folder) if f.endswith('.mp4')]\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "for vid in list_of_train_data:\n",
    "    count = 0\n",
    "    cap = cv2.VideoCapture(os.path.join(train_frame_folder, vid))\n",
    "    frameRate = cap.get(5)\n",
    "    while cap.isOpened():\n",
    "        frameId = cap.get(1)\n",
    "        ret, frame = cap.read()\n",
    "        if ret != True:\n",
    "            break\n",
    "        if frameId % ((int(frameRate)+1)*1) == 0:\n",
    "            face_rects, scores, idx = detector.run(frame, 0)\n",
    "            for i, d in enumerate(face_rects):\n",
    "                x1 = d.left()\n",
    "                y1 = d.top()\n",
    "                x2 = d.right()\n",
    "                y2 = d.bottom()\n",
    "                crop_img = frame[y1:y2, x1:x2]\n",
    "                if data[vid]['label'] == 'REAL':\n",
    "                    cv2.imwrite('dataset/real/'+vid.split('.')[0]+'_'+str(count)+'.png', cv2.resize(crop_img, (128, 128)))\n",
    "                elif data[vid]['label'] == 'FAKE':\n",
    "                    cv2.imwrite('dataset/fake/'+vid.split('.')[0]+'_'+str(count)+'.png', cv2.resize(crop_img, (128, 128)))\n",
    "                count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (128, 128, 3)\n",
    "data_dir = 'dataset'\n",
    "\n",
    "real_data = [f for f in os.listdir(data_dir+'/real') if f.endswith('.png')]\n",
    "fake_data = [f for f in os.listdir(data_dir+'/fake') if f.endswith('.png')]\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for img in real_data:\n",
    "    X.append(img_to_array(load_img(data_dir+'/real/'+img)).flatten() / 255.0)\n",
    "    Y.append(1)\n",
    "for img in fake_data:\n",
    "    X.append(img_to_array(load_img(data_dir+'/fake/'+img)).flatten() / 255.0)\n",
    "    Y.append(0)\n",
    "\n",
    "Y_val_org = Y\n",
    "\n",
    "#Normalization\n",
    "X = np.array(X)\n",
    "Y = to_categorical(Y, 2)\n",
    "\n",
    "#Reshape\n",
    "X = X.reshape(-1, 128, 128, 3)\n",
    "\n",
    "#Train-Test split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jiho/.venv/py369tf/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "#from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "resNet_model = InceptionResNetV2(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "resNet_model.trainable = True\n",
    "model = Sequential()\n",
    "model.add(resNet_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_resnet_v2 (Model)  (None, 2, 2, 1536)        54336736  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 3074      \n",
      "=================================================================\n",
      "Total params: 54,339,810\n",
      "Trainable params: 54,279,266\n",
      "Non-trainable params: 60,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer=optimizers.Adam(lr=1e-5, epsilon=None)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        '/home/jiho/work/repos/deepfake/model/weights.{epoch:02d}-{val_loss:.2f}.hdf5', \n",
    "        verbose=1),\n",
    "    keras.callbacks.TensorBoard(log_dir='/home/jiho/work/repos/deepfake')#그래프로 보기위해 기록 저장\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2996 samples, validate on 749 samples\n",
      "WARNING:tensorflow:From /home/jiho/.venv/py369tf/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "2900/2996 [============================>.] - ETA: 1s - loss: 0.5468 - acc: 0.7817\n",
      "Epoch 00001: saving model to /home/jiho/work/repos/deepfake/model/weights.01-0.61.hdf5\n",
      "2996/2996 [==============================] - 45s 15ms/sample - loss: 0.5444 - acc: 0.7830 - val_loss: 0.6052 - val_acc: 0.7664\n",
      "Epoch 2/20\n",
      "2900/2996 [============================>.] - ETA: 0s - loss: 0.4173 - acc: 0.8417\n",
      "Epoch 00002: saving model to /home/jiho/work/repos/deepfake/model/weights.02-0.55.hdf5\n",
      "2996/2996 [==============================] - 12s 4ms/sample - loss: 0.4163 - acc: 0.8435 - val_loss: 0.5456 - val_acc: 0.7850\n",
      "Epoch 3/20\n",
      "2900/2996 [============================>.] - ETA: 0s - loss: 0.3353 - acc: 0.8817\n",
      "Epoch 00003: saving model to /home/jiho/work/repos/deepfake/model/weights.03-0.51.hdf5\n",
      "2996/2996 [==============================] - 12s 4ms/sample - loss: 0.3334 - acc: 0.8842 - val_loss: 0.5091 - val_acc: 0.7837\n",
      "Epoch 4/20\n",
      "2900/2996 [============================>.] - ETA: 0s - loss: 0.2721 - acc: 0.9217\n",
      "Epoch 00004: saving model to /home/jiho/work/repos/deepfake/model/weights.04-0.47.hdf5\n",
      "2996/2996 [==============================] - 13s 4ms/sample - loss: 0.2717 - acc: 0.9209 - val_loss: 0.4700 - val_acc: 0.7997\n",
      "Epoch 5/20\n",
      "2900/2996 [============================>.] - ETA: 0s - loss: 0.2200 - acc: 0.9424\n",
      "Epoch 00005: saving model to /home/jiho/work/repos/deepfake/model/weights.05-0.48.hdf5\n",
      "2996/2996 [==============================] - 13s 4ms/sample - loss: 0.2191 - acc: 0.9426 - val_loss: 0.4802 - val_acc: 0.8211\n",
      "Epoch 6/20\n",
      "2900/2996 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9621\n",
      "Epoch 00006: saving model to /home/jiho/work/repos/deepfake/model/weights.06-0.48.hdf5\n",
      "2996/2996 [==============================] - 13s 4ms/sample - loss: 0.1751 - acc: 0.9619 - val_loss: 0.4826 - val_acc: 0.8238\n",
      "Epoch 7/20\n",
      "2900/2996 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.9759\n",
      "Epoch 00007: saving model to /home/jiho/work/repos/deepfake/model/weights.07-0.48.hdf5\n",
      "2996/2996 [==============================] - 13s 4ms/sample - loss: 0.1361 - acc: 0.9763 - val_loss: 0.4827 - val_acc: 0.8531\n",
      "Epoch 8/20\n",
      "2900/2996 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9797\n",
      "Epoch 00008: saving model to /home/jiho/work/repos/deepfake/model/weights.08-0.46.hdf5\n",
      "2996/2996 [==============================] - 13s 4ms/sample - loss: 0.1129 - acc: 0.9796 - val_loss: 0.4649 - val_acc: 0.8665\n",
      "Epoch 9/20\n",
      "2900/2996 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9876\n",
      "Epoch 00009: saving model to /home/jiho/work/repos/deepfake/model/weights.09-0.44.hdf5\n",
      "2996/2996 [==============================] - 13s 4ms/sample - loss: 0.0870 - acc: 0.9877 - val_loss: 0.4391 - val_acc: 0.8852\n",
      "Epoch 10/20\n",
      "2900/2996 [============================>.] - ETA: 0s - loss: 0.0684 - acc: 0.9900\n",
      "Epoch 00010: saving model to /home/jiho/work/repos/deepfake/model/weights.10-0.42.hdf5\n",
      "2996/2996 [==============================] - 13s 4ms/sample - loss: 0.0681 - acc: 0.9903 - val_loss: 0.4156 - val_acc: 0.8932\n",
      "Epoch 11/20\n",
      "2900/2996 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9921\n",
      "Epoch 00011: saving model to /home/jiho/work/repos/deepfake/model/weights.11-0.40.hdf5\n",
      "2996/2996 [==============================] - 13s 4ms/sample - loss: 0.0581 - acc: 0.9923 - val_loss: 0.4017 - val_acc: 0.8999\n",
      "Epoch 12/20\n",
      "2900/2996 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9938\n",
      "Epoch 00012: saving model to /home/jiho/work/repos/deepfake/model/weights.12-0.38.hdf5\n",
      "2996/2996 [==============================] - 13s 4ms/sample - loss: 0.0453 - acc: 0.9930 - val_loss: 0.3762 - val_acc: 0.9012\n",
      "Epoch 13/20\n",
      "2900/2996 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9934\n",
      "Epoch 00013: saving model to /home/jiho/work/repos/deepfake/model/weights.13-0.37.hdf5\n",
      "2996/2996 [==============================] - 13s 4ms/sample - loss: 0.0407 - acc: 0.9933 - val_loss: 0.3672 - val_acc: 0.8972\n",
      "Epoch 14/20\n",
      "2900/2996 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9938\n",
      "Epoch 00014: saving model to /home/jiho/work/repos/deepfake/model/weights.14-0.36.hdf5\n",
      "2996/2996 [==============================] - 13s 4ms/sample - loss: 0.0352 - acc: 0.9940 - val_loss: 0.3563 - val_acc: 0.9065\n",
      "Epoch 15/20\n",
      "2900/2996 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9931\n",
      "Epoch 00015: saving model to /home/jiho/work/repos/deepfake/model/weights.15-0.35.hdf5\n",
      "2996/2996 [==============================] - 13s 4ms/sample - loss: 0.0320 - acc: 0.9927 - val_loss: 0.3537 - val_acc: 0.9146\n",
      "Epoch 16/20\n",
      "2900/2996 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9948\n",
      "Epoch 00016: saving model to /home/jiho/work/repos/deepfake/model/weights.16-0.36.hdf5\n",
      "2996/2996 [==============================] - 13s 4ms/sample - loss: 0.0259 - acc: 0.9947 - val_loss: 0.3605 - val_acc: 0.9119\n",
      "Epoch 17/20\n",
      "2900/2996 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9966\n",
      "Epoch 00017: saving model to /home/jiho/work/repos/deepfake/model/weights.17-0.36.hdf5\n",
      "2996/2996 [==============================] - 13s 4ms/sample - loss: 0.0224 - acc: 0.9967 - val_loss: 0.3648 - val_acc: 0.9092\n",
      "Epoch 18/20\n",
      "2900/2996 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9948\n",
      "Epoch 00018: saving model to /home/jiho/work/repos/deepfake/model/weights.18-0.37.hdf5\n",
      "2996/2996 [==============================] - 13s 4ms/sample - loss: 0.0205 - acc: 0.9950 - val_loss: 0.3682 - val_acc: 0.9105\n",
      "Epoch 19/20\n",
      "2900/2996 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9962\n",
      "Epoch 00019: saving model to /home/jiho/work/repos/deepfake/model/weights.19-0.37.hdf5\n",
      "2996/2996 [==============================] - 13s 4ms/sample - loss: 0.0199 - acc: 0.9963 - val_loss: 0.3706 - val_acc: 0.9052\n",
      "Epoch 20/20\n",
      "2900/2996 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9952\n",
      "Epoch 00020: saving model to /home/jiho/work/repos/deepfake/model/weights.20-0.38.hdf5\n",
      "2996/2996 [==============================] - 13s 4ms/sample - loss: 0.0196 - acc: 0.9953 - val_loss: 0.3760 - val_acc: 0.9092\n"
     ]
    }
   ],
   "source": [
    "#Currently not used\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 100\n",
    "history = model.fit(X_train, Y_train, batch_size = BATCH_SIZE, epochs = EPOCHS, validation_data = (X_val, Y_val), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
